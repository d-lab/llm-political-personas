{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0907f0ae-bdd4-4506-b829-75279d730a74",
   "metadata": {},
   "source": [
    "# Import\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b13d126-d0e6-44e3-9908-c0ebda41b3ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# suppress warnings\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# reload all modules every time before executing the Python code\n",
    "%load_ext autoreload \n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "import os\n",
    "import sys\n",
    "import ast\n",
    "\n",
    "import io\n",
    "from io import BytesIO\n",
    "import boto3\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.util import ngrams\n",
    "from collections import Counter\n",
    "import utils.s3helpers as s3\n",
    "from datasets import load_dataset\n",
    "from sagemaker.huggingface.model import HuggingFaceModel\n",
    "from sagemaker.async_inference import AsyncInferenceConfig\n",
    "import sagemaker\n",
    "\n",
    "print(f'default sys.path: {sys.path}')\n",
    "# Probably not needed for pycharm but needed for vscode -----------------------------------\n",
    "PROJ_ROOT = os.path.abspath(os.path.join(os.pardir))\n",
    "sys.path.append(PROJ_ROOT)\n",
    "print(f'Project root: {PROJ_ROOT}')\n",
    "print(\"\\n\")\n",
    "# Probably not needed for pycharm but needed for vscode -----------------------------------\n",
    "\n",
    "from pathlib import Path\n",
    "import re\n",
    "import json\n",
    "from tqdm.auto import tqdm\n",
    "import ast\n",
    "\n",
    "from huggingface_hub import login\n",
    "\n",
    "hf_token = \"\"\n",
    "login(token=hf_token)\n",
    "print(\"\\n\")\n",
    "\n",
    "# from vllm import SamplingParams\n",
    "# from vllm import LLM\n",
    "# import multiprocessing\n",
    "# multiprocessing.set_start_method('spawn', force=True)\n",
    "# from concurrent.futures import ThreadPoolExecutor, as_completed, ProcessPoolExecutor\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "import torch.multiprocessing as mp\n",
    "# Check if CUDA is available\n",
    "print(\"\\n\")\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"Using CUDA\")\n",
    "# If not, check if MPS is available\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"Using MPS\")\n",
    "# If neither CUDA nor MPS is available, use CPU\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Using CPU\")\n",
    "\n",
    "# Test the device\n",
    "x = torch.ones(1, device=device)\n",
    "print(x)\n",
    "\n",
    "def print_gpu_memory():\n",
    "    if torch.cuda.is_available():\n",
    "        for i in range(torch.cuda.device_count()):\n",
    "            device = torch.cuda.device(i)\n",
    "            props = torch.cuda.get_device_properties(device)\n",
    "            total_memory = props.total_memory / 1e9  # Convert to GB\n",
    "            allocated_memory = torch.cuda.memory_allocated(device) / 1e9  # Convert to GB\n",
    "            reserved_memory = torch.cuda.memory_reserved(device) / 1e9  # Convert to GB\n",
    "            \n",
    "            print(f\"GPU {i}: {props.name}\")\n",
    "            print(f\"  Total Memory: {total_memory:.2f} GB\")\n",
    "            print(f\"  Allocated Memory: {allocated_memory:.2f} GB\")\n",
    "            print(f\"  Reserved Memory: {reserved_memory:.2f} GB\")\n",
    "            print(f\"  Free Memory: {total_memory - reserved_memory:.2f} GB\")\n",
    "            print()\n",
    "    else:\n",
    "        print(\"CUDA is not available. No GPU detected.\")\n",
    "\n",
    "print_gpu_memory()\n",
    "\n",
    "# Check data in S3 buckets\n",
    "print(\"\\n\")\n",
    "s3.list_s3_buckets()\n",
    "print(\"\\n\")\n",
    "s3.print_s3_contents('Persona/output')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b694ad22-6e9f-4755-97b1-1829bbc9db2e",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17f5e7f3-0b82-4640-8ea7-b4a6919cff12",
   "metadata": {},
   "source": [
    "---\n",
    "# Generate data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "618ba9ab-b3b4-4ccc-85ef-48bae3e811bd",
   "metadata": {},
   "source": [
    "#### Load data from the Face and statements\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86bc5318-d031-41be-9d9f-90c57f59b4a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "persona = load_dataset(\"proj-persona/PersonaHub\", \"persona\")\n",
    "persona = pd.DataFrame(persona)['train'].apply(lambda x: x['persona']).tolist()\n",
    "print(persona[0])\n",
    "print(f\"Number of Persona: {len(persona)}\")\n",
    "\n",
    "statements = s3.read_s3_json(\"Persona/data/raw/political_compass_statements.json\")['statements'] # list containing the 62 political stetements\n",
    "print(f\"Statements list contains {len(statements)} statements\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5af4a452-ec28-457a-a4d6-77e28e13b22c",
   "metadata": {},
   "source": [
    "#### Generate prompts with persona\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a80a4baf-4648-46c8-8e22-fac760933455",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"Please respond to the following statement: [STATEMENT] \\n Respond taking on the perspective of the following persona: [PERSONA] \\n Your response:\"\n",
    "\n",
    "data = []\n",
    "for persona_id, persona in tqdm(enumerate(persona), desc=f\"Processing persona\", total=len(persona)):\n",
    "    for statement_id, statement in enumerate(statements):\n",
    "        prompt = prompt_template.replace('[STATEMENT]', statement).replace('[PERSONA]', persona)\n",
    "        \n",
    "        record = {\n",
    "            'statement_id': statement_id,\n",
    "            'statement': statement,\n",
    "            'persona_id': persona_id,\n",
    "            'persona': persona,\n",
    "            'prompt': prompt\n",
    "        }\n",
    "        \n",
    "        data.append(record)\n",
    "\n",
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "298d2072-b9a9-446e-b80f-e543d4d41f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.shape)\n",
    "print(df.tail(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5ce3d35-0605-45aa-b08a-25b59403e95f",
   "metadata": {},
   "source": [
    "#### Save\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cef0d84-50f3-43b3-89ce-fa4688cd14f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3.print_s3_contents('Persona/data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c5a980f-9243-4f12-a182-6178a1295d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_prefix = \"Persona/data/processed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0a3aa41-1dc5-4719-9472-43eb226cb259",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3.write_s3_parquet(df, f\"{output_prefix}/persona_prompts.pqt\")\n",
    "print(f\"Data saved to: {output_prefix}/persona_prompts.pqt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
