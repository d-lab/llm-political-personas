{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c940042-4081-41f4-a7ab-8b9304a28362",
   "metadata": {},
   "source": [
    "# Initialisation\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09dcdc78-9ac1-433b-9c9d-af30c43da761",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q vllm\n",
    "!pip install -q outlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c246d507-3230-4c9c-b175-84f3f8c3c88c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%env VLLM_WORKER_MULTIPROC_METHOD=spawn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d56b3c3e-0abf-4f30-9199-ad70927b53f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import utils.s3helpers as s3\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "from contextlib import contextmanager\n",
    "\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "import multiprocessing\n",
    "from typing import List, Tuple\n",
    "from time import time\n",
    "from datetime import datetime\n",
    "from vllm import LLM, SamplingParams\n",
    "from outlines import models, generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d58d27c8-7c64-4e3d-b648-2916b18cff17",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "hf_token = \"\"\n",
    "login(token=hf_token)\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1caa8c11-e6a4-441b-82bc-be08d31fe31e",
   "metadata": {},
   "source": [
    "---\n",
    "# Answer generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7027a992-082d-4364-a366-76beecc18e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read from s3 the previously created dataframe with the 12.400.000 prompts (200k persona * 62 political statements)\n",
    "df = s3.read_s3_parquet(f\"Persona/data/interim/pct_persona_prompts_righta.pqt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c2295a-2a22-42c8-a86b-517c83525ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(df))\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a8bd7bc-9d80-4ea2-8105-d6ffd3f1fe79",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.iloc[61]['prompt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1331892-fd7d-4232-8978-656782c279c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = df['prompt'].tolist()\n",
    "print(len(prompts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d398f94-5cfd-4de7-af52-a75926a6d3c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialising which model to use and where to save the resulting dataframe\n",
    "# mistralai/Mistral-7B-Instruct-v0.3 - meta-llama/Llama-3.1-8B-Instruct\",\"HuggingFaceH4/zephyr-7b-beta\",\"Qwen/Qwen2.5-7B-Instruct\"]\n",
    "\n",
    "model_id = \"zephyr-7b-beta\"\n",
    "model = \"HuggingFaceH4/zephyr-7b-beta\"\n",
    "print(f\"Using: {model}\")\n",
    "\n",
    "output_prefix = f\"Persona/output/persona_distribution/{model_id}/right_authoritarian_personas\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f29cc3c-9f9c-4a26-be92-4a32621e9738",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_GPUS = torch.cuda.device_count()\n",
    "print(f\"Number of GPUs: {NUM_GPUS}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dc9b8477-a715-41f4-a858-c451d43ec8b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# code to split the batch of prompts, instanciate the LLMs over the GPUs and run the inference in parallel\n",
    "def run_inference_one_gpu(gpu_id, prompt_list, model_name):\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(gpu_id)\n",
    "    prompts_array = np.array(prompt_list)\n",
    "    llm = LLM(\n",
    "        model=model_name, \n",
    "        tokenizer_mode=\"auto\", # auto - mistral\n",
    "        trust_remote_code=True,\n",
    "        enable_chunked_prefill=True,\n",
    "        # enable_prefix_caching=True,\n",
    "    )\n",
    "    \n",
    "    model = models.VLLM(llm)\n",
    "    generator_choice = generate.choice(model, [\"Disagree\", \"Agree\", \"Strongly disagree\", \"Strongly agree\"])\n",
    "    output = generator_choice(prompts_array)\n",
    "    \n",
    "    return output\n",
    "\n",
    "\n",
    "split_list = lambda l, n: [l[i * len(l) // n: (i + 1) * len(l) // n] for i in range(n)]\n",
    "\n",
    "def run_inference_multi_gpu(model_name, prompts):\n",
    "    split_prompts = split_list(prompts, NUM_GPUS)\n",
    "    inputs = [(i, p, model_name) for i, p in enumerate(split_prompts)]\n",
    "\n",
    "    with multiprocessing.Pool(processes=NUM_GPUS) as pool:\n",
    "        results = pool.starmap(run_inference_one_gpu, inputs)\n",
    "\n",
    "    outputs = []\n",
    "    for result in results:\n",
    "        outputs.extend(result)\n",
    "\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "759e7c9a-024f-4e47-8c43-78e38f8a4a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# code that start the inference process (I decided to split every 10000 personas so that I can check the output every now and then without waiting for the whole process to finish to discover errors, to try out stuff you can also set it lower (10))\n",
    "%%capture\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    model_name = model\n",
    "    batch_size = 10000 # peronas per batch\n",
    "    number_personas = len(prompts)//62\n",
    "    number_of_batches = number_personas // batch_size\n",
    "\n",
    "    for i in tqdm(range(13, number_of_batches)):\n",
    "        start_idx = i * (batch_size * 62)\n",
    "        end_idx = start_idx + (batch_size * 62)\n",
    "        sub_df = df[start_idx:end_idx].copy()\n",
    "        sub_prompts = sub_df['prompt'].tolist()\n",
    "\n",
    "        raw_responses = run_inference_multi_gpu(model_name, sub_prompts)\n",
    "        sub_df['response'] = raw_responses\n",
    "        print(f\"Saving batch number {i}\")\n",
    "        s3.write_s3_parquet(sub_df, f\"{output_prefix}/sub_dfs_stanced/df_b{i}_p{batch_size}.pqt\")\n",
    "        print(f\"Data saved to: {output_prefix}/sub_dfs_stanced/df_b{i}_p{batch_size}.pqt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebf339ab-7973-448d-b30b-178eee33ce34",
   "metadata": {},
   "source": [
    "Single persona prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56823d9d-e55e-4026-95ad-ea8ee18f3aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    model_name = model\n",
    "    raw_responses = run_inference_multi_gpu(model_name, prompts)\n",
    "    df['response'] = raw_responses\n",
    "    s3.write_s3_parquet(df, f\"{output_prefix}/compass_autr.pqt\")\n",
    "    print(f\"Data saved to: {output_prefix}/compass_autr.pqt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
