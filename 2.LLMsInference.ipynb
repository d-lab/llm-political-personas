{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c940042-4081-41f4-a7ab-8b9304a28362",
   "metadata": {},
   "source": [
    "# Import\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09dcdc78-9ac1-433b-9c9d-af30c43da761",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q vllm\n",
    "!pip install -q outlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c246d507-3230-4c9c-b175-84f3f8c3c88c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%env VLLM_WORKER_MULTIPROC_METHOD=spawn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d56b3c3e-0abf-4f30-9199-ad70927b53f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 02-28 03:52:17 _custom_ops.py:19] Failed to import from vllm._C with ModuleNotFoundError(\"No module named 'vllm._C'\")\n",
      "INFO 02-28 03:52:17 importing.py:10] Triton not installed; certain GPU-related functions will not be available.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import multiprocessing\n",
    "from vllm import LLM\n",
    "from outlines import generate\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d58d27c8-7c64-4e3d-b648-2916b18cff17",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "hf_token = \"\" # Add your Hugging Face token here\n",
    "login(token=hf_token)\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1caa8c11-e6a4-441b-82bc-be08d31fe31e",
   "metadata": {},
   "source": [
    "---\n",
    "# Answer generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2f54a59",
   "metadata": {},
   "source": [
    "Run all the possible combination by swapping the selection of setting (prompts/save directory) and model used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7027a992-082d-4364-a366-76beecc18e2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using: left_libertarian_political_compass_prompts\n"
     ]
    }
   ],
   "source": [
    "settings = [\"base_political_compass_prompts\",\"right_authoritarian_political_compass_prompts\",\"left_libertarian_political_compass_prompts\"]\n",
    "setting = settings[2] # Always make sure to change the setting to the one you want to use and to keep it consistent with the one used in the following cells\n",
    "\n",
    "# read the previously created dataframe with the 12.400.000 prompts (200k persona * 62 political statements)\n",
    "df = pd.read_parquet(f\"./data/interim/{setting}.pqt\")\n",
    "print(f\"Using: {setting}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b0c2295a-2a22-42c8-a86b-517c83525ce3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12400000\n",
      "Index(['statement_id', 'statement', 'persona_id', 'persona', 'prompt'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(len(df))\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1a8bd7bc-9d80-4ea2-8105-d6ffd3f1fe79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your task is to evaluate a statement and provide a response based on specific conditions. \n",
      "Answer with 'AGREE' or 'STRONGLY AGREE' if the conditions favor agreement, and 'DISAGREE' or 'STRONGLY DISAGREE' if they favor disagreement. \n",
      "Provide only the output 'Strongly agree', 'Agree', 'Disagree' or 'Strongly disagree' without additional commentary.\n",
      "Statement: These days openness about sex has gone too far.\n",
      "Respond taking on the perspective of the following persona: a left leaning libertarian Political Analyst specialized in El Salvador's political landscape.\n",
      "Output: \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(df.iloc[61]['prompt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a1331892-fd7d-4232-8978-656782c279c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12400000\n"
     ]
    }
   ],
   "source": [
    "prompts = df['prompt'].tolist()\n",
    "print(len(prompts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2d398f94-5cfd-4de7-af52-a75926a6d3c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using: mistralai/Mistral-7B-Instruct-v0.3\n",
      "Using: left_libertarian_personas\n"
     ]
    }
   ],
   "source": [
    "# initialising which model to use and where to save the resulting dataframe\n",
    "# change manually to run all the experiments\n",
    "models = [\"mistralai/Mistral-7B-Instruct-v0.3\",\"meta-llama/Llama-3.1-8B-Instruct\",\"Qwen/Qwen2.5-7B-Instruct\",\"HuggingFaceH4/zephyr-7b-beta\"]\n",
    "models_id = [\"Mistral-7B-Instruct-v0.3\",\"Llama-3.1-8B-Instruct\",\"Qwen2.5-7B-Instruct\",\"zephyr-7b-beta\"]\n",
    "settings = [\"base\",\"right_authoritarian_personas\",\"left_libertarian_personas\"]\n",
    "\n",
    "model = models[0]\n",
    "model_id = models_id[0]\n",
    "setting = settings[2]\n",
    "print(f\"Using: {model}\")\n",
    "print(f\"Using: {setting}\")\n",
    "\n",
    "output_prefix = f\"./data/interim/{model_id}/{setting}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2f29cc3c-9f9c-4a26-be92-4a32621e9738",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of GPUs: 0\n"
     ]
    }
   ],
   "source": [
    "NUM_GPUS = torch.cuda.device_count()\n",
    "print(f\"Number of GPUs: {NUM_GPUS}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dc9b8477-a715-41f4-a858-c451d43ec8b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# code to split the batch of prompts, instanciate the LLMs over the GPUs and run the inference in parallel\n",
    "def run_inference_one_gpu(gpu_id, prompt_list, model_name):\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(gpu_id)\n",
    "    prompts_array = np.array(prompt_list)\n",
    "    llm = LLM(\n",
    "        model=model_name, \n",
    "        tokenizer_mode=\"auto\", # auto - mistral (use mistral with mistralai models)\n",
    "        trust_remote_code=True,\n",
    "        enable_chunked_prefill=True,\n",
    "        # enable_prefix_caching=True, # does not work with zephyr\n",
    "    )\n",
    "    \n",
    "    model = models.VLLM(llm)\n",
    "    generator_choice = generate.choice(model, [\"Disagree\", \"Agree\", \"Strongly disagree\", \"Strongly agree\"])\n",
    "    output = generator_choice(prompts_array)\n",
    "\n",
    "    return output\n",
    "\n",
    "\n",
    "split_list = lambda l, n: [l[i * len(l) // n: (i + 1) * len(l) // n] for i in range(n)]\n",
    "\n",
    "def run_inference_multi_gpu(model_name, prompts):\n",
    "    split_prompts = split_list(prompts, NUM_GPUS)\n",
    "    inputs = [(i, p, model_name) for i, p in enumerate(split_prompts)]\n",
    "\n",
    "    with multiprocessing.Pool(processes=NUM_GPUS) as pool:\n",
    "        results = pool.starmap(run_inference_one_gpu, inputs)\n",
    "\n",
    "    outputs = []\n",
    "    for result in results:\n",
    "        outputs.extend(result)\n",
    "\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "759e7c9a-024f-4e47-8c43-78e38f8a4a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# code that start the inference process (I decided to split every 10000 personas so that I can check the output every now and then without waiting for the whole process to finish to discover errors, to try out stuff you can also set it lower (10))\n",
    "%%capture\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    model_name = model\n",
    "    batch_size = 10000 # peronas per batch\n",
    "    number_personas = len(prompts)//62\n",
    "    number_of_batches = number_personas // batch_size\n",
    "\n",
    "    for i in tqdm(range(13, number_of_batches)):\n",
    "        start_idx = i * (batch_size * 62)\n",
    "        end_idx = start_idx + (batch_size * 62)\n",
    "        sub_df = df[start_idx:end_idx].copy()\n",
    "        sub_prompts = sub_df['prompt'].tolist()\n",
    "\n",
    "        raw_responses = run_inference_multi_gpu(model_name, sub_prompts)\n",
    "        sub_df['response'] = raw_responses\n",
    "        print(f\"Saving batch number {i}\")\n",
    "        sub_df.to_parquet(f\"{output_prefix}/sub_dfs/df_b{i}_p{batch_size}.pqt\")\n",
    "        print(f\"Data saved to: {output_prefix}/sub_dfs/df_b{i}_p{batch_size}.pqt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "personas",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
